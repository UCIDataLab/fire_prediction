{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GFS Open Question Exploration (v2.0)\n",
    "Casey A Graff\n",
    "\n",
    "August 11th, 2017\n",
    "\n",
    "**Now using re-fetched gfs data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REP_DIR = \"/home/cagraff/Documents/dev/fire_prediction/\"\n",
    "SRC_DIR = REP_DIR + 'src/'\n",
    "DATA_DIR = REP_DIR + 'data/'\n",
    "\n",
    "# Load system-wide packages\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from scipy.stats.stats import pearsonr\n",
    "import datetime as dt\n",
    "import pytz\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load project packages\n",
    "os.chdir(SRC_DIR)\n",
    "from features.loaders import load_gfs_weather\n",
    "from features.helper.daymonth import monthday2day, day2monthday\n",
    "from features.helper import date_util as du\n",
    "from data.grib import latlonrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "sys.path.append(SRC_DIR+'features')\n",
    "gfs = load_gfs_weather(os.path.join(DATA_DIR, 'interim/gfs/weather/alaska_2007-2011.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print gfs.cubes.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing files\n",
    "How many files are missing and is there a temporal pattern to the missing days?\n",
    "\n",
    "### Number of missings files per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2007, 2012)\n",
    "\n",
    "missing = []\n",
    "for year in years:\n",
    "    all_dates = [d for d in du.daterange(dt.datetime(year,1,1, tzinfo=pytz.UTC), dt.datetime(year+1, 1, 1, tzinfo=pytz.UTC), increment=dt.timedelta(hours=6))]\n",
    "\n",
    "    files_present = 0\n",
    "    files_missing = 0\n",
    "    for date in all_dates:\n",
    "        vals = gfs['temperature'].get_attribute_for_date('offsets', date)\n",
    "        \n",
    "        files_present += len(vals)\n",
    "        files_missing += 3-len(vals)\n",
    "        \n",
    "    missing.append((year, files_present, files_missing))\n",
    "\n",
    "# Missing total\n",
    "missing.append(('Total', sum([x[1] for x in missing]), sum([x[2] for x in missing])))\n",
    "print tabulate(missing, headers=['Year', 'Present', 'Missing'])\n",
    "\n",
    "print '\\nPercentage missing is {}%'.format(missing[-1][2]/(.01*missing[-1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of missings files per year (within fire season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2007, 2012)\n",
    "season = ((5,14), (8,31))\n",
    "\n",
    "print 'Fire Season:', season[0], 'to', season[1], '\\n'\n",
    "\n",
    "# Missing per year\n",
    "missing = []\n",
    "for year in years:\n",
    "    all_dates = [d for d in du.daterange(dt.datetime(year, season[0][0], season[0][1], tzinfo=pytz.UTC),\n",
    "                                         dt.datetime(year, season[1][0], season[1][1], tzinfo=pytz.UTC) + du.inc_one_day, increment=dt.timedelta(hours=6))]\n",
    "\n",
    "    files_present = 0\n",
    "    files_missing = 0\n",
    "    for date in all_dates:\n",
    "        vals = gfs['temperature'].get_attribute_for_date('offsets', date)\n",
    "        \n",
    "        files_present += len(vals)\n",
    "        files_missing += 3-len(vals)\n",
    "        \n",
    "    missing.append((year, files_present, files_missing))\n",
    "\n",
    "# Missing total\n",
    "missing.append(('Total', sum([x[1] for x in missing]), sum([x[2] for x in missing])))\n",
    "print tabulate(missing, headers=['Year', 'Present', 'Missing'])\n",
    "\n",
    "print '\\nPercentage missing is {}%'.format(missing[-1][2]/(.01*missing[-1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of missing days per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2007, 2012)\n",
    "months = range(1, 13)\n",
    "\n",
    "MONTH_IND = 0\n",
    "PRESENT_IND = 1\n",
    "MISSING_IND = 2\n",
    "\n",
    "missing = np.zeros((12,3), dtype=np.int32)\n",
    "missing[:, MONTH_IND] = months\n",
    "        \n",
    "for year in years:\n",
    "    for month in months:\n",
    "        month_num_days = du.days_per_month(month, du.is_leap_year(year))\n",
    "        all_dates = [d for d in du.daterange(dt.datetime(year, month, 1, tzinfo=pytz.UTC),\n",
    "                                         dt.datetime(year, month, month_num_days, tzinfo=pytz.UTC) + du.inc_one_day, increment=dt.timedelta(hours=6))]\n",
    "\n",
    "        files_present = 0\n",
    "        files_missing = 0\n",
    "        for date in all_dates:\n",
    "            vals = gfs['temperature'].get_attribute_for_date('offsets', date)\n",
    "\n",
    "            files_present += len(vals)\n",
    "            files_missing += 3-len(vals)\n",
    "        \n",
    "        missing[month-1,PRESENT_IND] += files_present\n",
    "        missing[month-1,MISSING_IND] += files_missing\n",
    "\n",
    "\n",
    "# Missing total\n",
    "present_total = np.sum(missing[:, PRESENT_IND])\n",
    "missing_total = np.sum(missing[:, MISSING_IND])\n",
    "missing = list(missing)\n",
    "missing.append(['Total', present_total, missing_total])\n",
    "\n",
    "print tabulate(missing, headers=['Month', 'Present', 'Missing'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Missing Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2011, 2012)\n",
    "grib_file_fmt = \"gfsanl_4_%s%.2d%.2d_%.2d%.2d_%.3d.grb2\"\n",
    "\n",
    "missing_files = []\n",
    "for year in years:\n",
    "    all_dates = [d for d in du.daterange(dt.datetime(year,1,1, tzinfo=pytz.UTC), dt.datetime(year+1, 1, 1, tzinfo=pytz.UTC), increment=dt.timedelta(hours=6))]\n",
    "\n",
    "    for date in all_dates:\n",
    "        offsets_found = gfs['temperature'].get_attribute_for_date('offsets', date)\n",
    "        \n",
    "        gribs_found = [grib_file_fmt % (year, date.month, date.day, date.hour, date.minute, offset.seconds/3600) for offset in offsets_found]\n",
    "        gribs_expected = [grib_file_fmt % (year, date.month, date.day, date.hour, date.minute, offset) for offset in (0, 3, 6)]\n",
    "\n",
    "        missing_files += list(set(gribs_expected).difference(set(gribs_found)))\n",
    "\n",
    "print len(missing_files), missing_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Correlation of Measurements\n",
    "\n",
    "Do adjacent pixel have a high correlation between measurements? If there is sufficient variability it may be useful to perform linear interpolation between neighboring cells when calculating the weather variables for a fire event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10,15]\n",
    "\n",
    "def make_map():\n",
    "    lat_min, lat_max, lon_min, lon_max = gfs['temperature'].bounding_box.get()\n",
    "\n",
    "    print (lat_min, lat_max), (lon_min, lon_max)\n",
    "\n",
    "    mp = Basemap(projection=\"merc\",\n",
    "                  llcrnrlat=lat_min,\n",
    "                  llcrnrlon=lon_min,\n",
    "                  urcrnrlat=lat_max,\n",
    "                  urcrnrlon=lon_max,\n",
    "                  resolution='i')\n",
    "\n",
    "    mp.drawcoastlines()\n",
    "    #mp.drawlsmask()\n",
    "\n",
    "    parallels = np.arange(lat_min,lat_max,2)\n",
    "    _ = mp.drawparallels(parallels,labels=[False,True,False,False])\n",
    "    \n",
    "    parallels = np.arange(lat_min,lat_max,.5)\n",
    "    _ = mp.drawparallels(parallels,labels=[False,False,False,False])\n",
    "\n",
    "    meridians = np.arange(lon_min,lon_max,2)\n",
    "    _ = mp.drawmeridians(meridians, labels=[False,False,False,True])\n",
    "    \n",
    "    meridians = np.arange(lon_min,lon_max,.5)\n",
    "    _ = mp.drawmeridians(meridians, labels=[False,False,False,False])\n",
    "    \n",
    "    return mp\n",
    "\n",
    "mp = make_map()\n",
    "\n",
    "latlon = [ll for ll in latlonrange(gfs['temperature'].bounding_box, .5, .5)]\n",
    "lats,lons = zip(*latlon)\n",
    "_ = mp.scatter(lons, lats ,30, latlon=True, marker='o', color='b')\n",
    "_ = plt.title('GFS Meaurement Points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = ['total_precipitation','u_wind_component', 'v_wind_component', 'temperature', 'humidity']\n",
    "DATA_TYPE = data_types[0]\n",
    "DATE_SEL = dt.datetime(2009, 3, 5, 18, tzinfo=pytz.UTC)\n",
    "OFFSET_SEL = 2\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10,15]\n",
    "\n",
    "mp = make_map()\n",
    "mp.shadedrelief()\n",
    "\n",
    "values = gfs[DATA_TYPE].get_values_for_date(DATE_SEL)[:,:,OFFSET_SEL]\n",
    "num_lats, num_lons = values.shape[0], values.shape[1]\n",
    "lons, lats = mp.makegrid(num_lons, num_lats)\n",
    "lats = np.transpose(np.tile(np.arange(71,54.5, -.5), (lons.shape[1],1) ))\n",
    "\n",
    "cs = mp.contourf(lons, lats , values, latlon=True, alpha=.6)\n",
    "cbar = mp.colorbar(cs,location='bottom',pad=\"5%\")\n",
    "#cbar.set_label('Kelvin (degrees)')\n",
    "\n",
    "_ = plt.title('%s at %s' % (DATA_TYPE, DATE_SEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean(data_type):\n",
    "    shape = gfs[data_type].values.shape\n",
    "    table = []\n",
    "    for data in [data_type]:\n",
    "        mean = np.zeros(shape[:2])\n",
    "        for lat in range(0, shape[0]):\n",
    "            for lon in range(0, shape[1]):\n",
    "                v = gfs[data].values[lat, lon]\n",
    "                \n",
    "                # Remove nans\n",
    "                v = v[np.logical_not(np.isnan(v))]\n",
    "                \n",
    "                mean[lat,lon] = np.mean(v)\n",
    "    return mean\n",
    "\n",
    "\n",
    "mp = make_map()\n",
    "mp.shadedrelief()\n",
    "\n",
    "values = get_mean(DATA_TYPE)\n",
    "num_lats, num_lons = values.shape[0], values.shape[1]\n",
    "lons, lats = mp.makegrid(num_lons, num_lats)\n",
    "lats = np.transpose(np.tile(np.arange(71,54.5, -.5), (lons.shape[1],1) ))\n",
    "\n",
    "cs = mp.contourf(lons, lats , values, latlon=True, alpha=.6)\n",
    "cbar = mp.colorbar(cs,location='bottom',pad=\"5%\")\n",
    "#cbar.set_label('Kelvin (degrees)')\n",
    "\n",
    "_ = plt.title('Mean %s' % DATA_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cor(lat_off_tup, lon_off_tup, data_type):\n",
    "    min_lat_off, max_lat_off, lat_off = lat_off_tup\n",
    "    min_lon_off, max_lon_off, lon_off = lon_off_tup\n",
    "    shape = gfs[data_type].values.shape\n",
    "    table = []\n",
    "    for data in [data_type]:\n",
    "        cor = np.zeros(shape[:2])\n",
    "        for lat in range(min_lat_off, shape[0] + max_lat_off):\n",
    "            for lon in range(min_lon_off, shape[1] + max_lon_off):\n",
    "                v = gfs[data].values[lat, lon]\n",
    "                v_off = gfs[data].values[lat+lat_off, lon+lon_off]\n",
    "                \n",
    "                # Remove nans\n",
    "                v = v[np.logical_not(np.isnan(v))]\n",
    "                v_off = v_off[np.logical_not(np.isnan(v_off))]\n",
    "                \n",
    "                cor[lat,lon] = pearsonr(v, v_off)[0]\n",
    "    return cor\n",
    "                \n",
    "# Calculate correlation for left neighbor\n",
    "cor = calc_cor((0, 0, 0), (1, 0, -1), DATA_TYPE)\n",
    "\n",
    "\n",
    "mp = make_map()\n",
    "mp.shadedrelief()\n",
    "\n",
    "values = cor\n",
    "num_lats, num_lons = values.shape[0], values.shape[1]\n",
    "lons, lats = mp.makegrid(num_lons, num_lats)\n",
    "lats = np.transpose(np.tile(np.arange(71,54.5, -.5), (lons.shape[1],1) ))\n",
    "\n",
    "cs = mp.contourf(lons[:,1:], lats[:,1:] ,values[:,1:], latlon=True, alpha=.6)\n",
    "cbar = mp.colorbar(cs,location='bottom',pad=\"5%\")\n",
    "cbar.set_label('Correlation (0 to 1)')\n",
    "\n",
    "_ = plt.title('Left Correlation of %s' % DATA_TYPE)\n",
    "\n",
    "print 'Mean Correlation %f' % np.mean(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation for top neighbor\n",
    "cor = calc_cor((1, 0, -1), (0, 0, 0), DATA_TYPE)\n",
    "\n",
    "mp = make_map()\n",
    "mp.shadedrelief()\n",
    "\n",
    "values = cor\n",
    "num_lats, num_lons = values.shape[0], values.shape[1]\n",
    "lons, lats = mp.makegrid(num_lons, num_lats)\n",
    "lats = np.transpose(np.tile(np.arange(71,54.5, -.5), (lons.shape[1],1) ))\n",
    "\n",
    "cs = mp.contourf(lons[1:, :], lats[1:, :] ,values[1:, :], latlon=True, alpha=.6)\n",
    "cbar = mp.colorbar(cs,location='bottom',pad=\"5%\")\n",
    "cbar.set_label('Correlation (0 to 1)')\n",
    "\n",
    "_ = plt.title('Top Correlation of %s' % DATA_TYPE)\n",
    "\n",
    "print 'Mean Correlation %f' % np.mean(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation for bottom neighbor\n",
    "cor = calc_cor((0, -1, 1), (0, 0, 0), DATA_TYPE)\n",
    "\n",
    "mp = make_map()\n",
    "mp.shadedrelief()\n",
    "\n",
    "values = cor\n",
    "num_lats, num_lons = values.shape[0], values.shape[1]\n",
    "lons, lats = mp.makegrid(num_lons, num_lats)\n",
    "lats = np.transpose(np.tile(np.arange(71,54.5, -.5), (lons.shape[1],1) ))\n",
    "\n",
    "cs = mp.contourf(lons[:-1, :], lats[:-1, :] ,values[:-1, :], latlon=True, alpha=.6)\n",
    "cbar = mp.colorbar(cs,location='bottom',pad=\"5%\")\n",
    "cbar.set_label('Correlation (0 to 1)')\n",
    "\n",
    "_ = plt.title('Bottom Correlation of %s' % DATA_TYPE)\n",
    "\n",
    "print 'Mean Correlation %f' % np.mean(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
