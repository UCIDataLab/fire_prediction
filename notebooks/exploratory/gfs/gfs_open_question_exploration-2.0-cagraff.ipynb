{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GFS Open Question Exploration (v2.0)\n",
    "Casey A Graff\n",
    "\n",
    "August 11th, 2017\n",
    "\n",
    "**Now using re-fetched gfs data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REP_DIR = \"/home/cagraff/Documents/dev/fire_prediction/\"\n",
    "SRC_DIR = REP_DIR + 'src/'\n",
    "DATA_DIR = REP_DIR + 'data/'\n",
    "\n",
    "# Load system-wide packages\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from scipy.stats.stats import pearsonr\n",
    "import datetime as dt\n",
    "import pytz\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load project packages\n",
    "os.chdir(SRC_DIR)\n",
    "from features.loaders import load_gfs_weather\n",
    "from features.helper import date_util as du\n",
    "from visualization.mapping import make_map\n",
    "from visualization.stats import calc_mean, calc_cor\n",
    "from data.grib import latlonrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "sys.path.append(SRC_DIR+'features')\n",
    "gfs = load_gfs_weather(os.path.join(DATA_DIR, 'interim/gfs/weather/weather_alaska_2007-2016.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print gfs.cubes.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing files\n",
    "How many files are missing and is there a temporal pattern to the missing days?\n",
    "\n",
    "### Number of missings files per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2007, 2017)\n",
    "\n",
    "missing = []\n",
    "for year in years:\n",
    "    all_dates = [d for d in du.daterange(dt.datetime(year,1,1, tzinfo=pytz.UTC), dt.datetime(year+1, 1, 1, tzinfo=pytz.UTC), increment=dt.timedelta(hours=6))]\n",
    "\n",
    "    files_present = 0\n",
    "    files_missing = 0\n",
    "    for date in all_dates:\n",
    "        vals = gfs['temperature'].get_attribute_for_date('offsets', date)\n",
    "        \n",
    "        files_present += len(vals)\n",
    "        files_missing += 3-len(vals)\n",
    "        \n",
    "    missing.append((year, files_present, files_missing))\n",
    "\n",
    "# Missing total\n",
    "missing.append(('Total', sum([x[1] for x in missing]), sum([x[2] for x in missing])))\n",
    "print tabulate(missing, headers=['Year', 'Present', 'Missing'])\n",
    "\n",
    "print '\\nPercentage missing is {}%'.format(missing[-1][2]/(.01*missing[-1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of missings files per year (within fire season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2007, 2017)\n",
    "season = ((5,14), (8,31))\n",
    "\n",
    "print 'Fire Season:', season[0], 'to', season[1], '\\n'\n",
    "\n",
    "# Missing per year\n",
    "missing = []\n",
    "for year in years:\n",
    "    all_dates = [d for d in du.daterange(dt.datetime(year, season[0][0], season[0][1], tzinfo=pytz.UTC),\n",
    "                                         dt.datetime(year, season[1][0], season[1][1], tzinfo=pytz.UTC) + du.INC_ONE_DAY, increment=dt.timedelta(hours=6))]\n",
    "\n",
    "    files_present = 0\n",
    "    files_missing = 0\n",
    "    for date in all_dates:\n",
    "        vals = gfs['temperature'].get_attribute_for_date('offsets', date)\n",
    "        \n",
    "        files_present += len(vals)\n",
    "        files_missing += 3-len(vals)\n",
    "        \n",
    "    missing.append((year, files_present, files_missing))\n",
    "\n",
    "# Missing total\n",
    "missing.append(('Total', sum([x[1] for x in missing]), sum([x[2] for x in missing])))\n",
    "print tabulate(missing, headers=['Year', 'Present', 'Missing'])\n",
    "\n",
    "print '\\nPercentage missing is {}%'.format(missing[-1][2]/(.01*missing[-1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of missing days per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2007, 2012)\n",
    "months = range(1, 13)\n",
    "\n",
    "MONTH_IND = 0\n",
    "PRESENT_IND = 1\n",
    "MISSING_IND = 2\n",
    "\n",
    "missing = np.zeros((12,3), dtype=np.int32)\n",
    "missing[:, MONTH_IND] = months\n",
    "        \n",
    "for year in years:\n",
    "    for month in months:\n",
    "        month_num_days = du.days_per_month(month, du.is_leap_year(year))\n",
    "        all_dates = [d for d in du.daterange(dt.datetime(year, month, 1, tzinfo=pytz.UTC),\n",
    "                                         dt.datetime(year, month, month_num_days, tzinfo=pytz.UTC) + du.INC_ONE_DAY, increment=dt.timedelta(hours=6))]\n",
    "\n",
    "        files_present = 0\n",
    "        files_missing = 0\n",
    "        for date in all_dates:\n",
    "            vals = gfs['temperature'].get_attribute_for_date('offsets', date)\n",
    "\n",
    "            files_present += len(vals)\n",
    "            files_missing += 3-len(vals)\n",
    "        \n",
    "        missing[month-1,PRESENT_IND] += files_present\n",
    "        missing[month-1,MISSING_IND] += files_missing\n",
    "\n",
    "\n",
    "# Missing total\n",
    "present_total = np.sum(missing[:, PRESENT_IND])\n",
    "missing_total = np.sum(missing[:, MISSING_IND])\n",
    "missing = list(missing)\n",
    "missing.append(['Total', present_total, missing_total])\n",
    "\n",
    "print tabulate(missing, headers=['Month', 'Present', 'Missing'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Missing Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2011, 2011)\n",
    "grib_file_fmt = \"gfsanl_4_%s%.2d%.2d_%.2d%.2d_%.3d.grb2\"\n",
    "\n",
    "missing_files = []\n",
    "for year in years:\n",
    "    all_dates = [d for d in du.daterange(dt.datetime(year,1,1, tzinfo=pytz.UTC), dt.datetime(year+1, 1, 1, tzinfo=pytz.UTC), increment=dt.timedelta(hours=6))]\n",
    "\n",
    "    for date in all_dates:\n",
    "        offsets_found = gfs['temperature'].get_attribute_for_date('offsets', date)\n",
    "        \n",
    "        gribs_found = [grib_file_fmt % (year, date.month, date.day, date.hour, date.minute, offset.seconds/3600) for offset in offsets_found]\n",
    "        gribs_expected = [grib_file_fmt % (year, date.month, date.day, date.hour, date.minute, offset) for offset in (0, 3, 6)]\n",
    "\n",
    "        missing_files += list(set(gribs_expected).difference(set(gribs_found)))\n",
    "\n",
    "print len(missing_files), missing_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Correlation of Measurements\n",
    "\n",
    "Do adjacent pixel have a high correlation between measurements? If there is sufficient variability it may be useful to perform linear interpolation between neighboring cells when calculating the weather variables for a fire event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = ['total_precipitation','u_wind_component', 'v_wind_component', 'temperature', 'humidity']\n",
    "DATA_TYPE = data_types[3]\n",
    "DATE_SEL = dt.datetime(2009, 3, 5, 18, tzinfo=pytz.UTC)\n",
    "OFFSET_SEL = 2\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10,15]\n",
    "\n",
    "mp = make_map(gfs[DATA_TYPE].bounding_box)\n",
    "mp.shadedrelief()\n",
    "\n",
    "latlon = [ll for ll in latlonrange(gfs[DATA_TYPE].bounding_box, .5, .5)]\n",
    "lats,lons = zip(*latlon)\n",
    "\n",
    "_ = mp.scatter(lons, lats ,30, latlon=True, marker='o', color='b')\n",
    "_ = plt.title('GFS Meaurement Points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "    \n",
    "mp = make_map(gfs[DATA_TYPE].bounding_box)\n",
    "mp.shadedrelief()\n",
    "\n",
    "values = gfs[DATA_TYPE].get_values_for_date(DATE_SEL)[:,:,OFFSET_SEL]\n",
    "lats, lons = gfs[DATA_TYPE].bounding_box.make_grid()\n",
    "\n",
    "cs = mp.contourf(lons, lats , values, latlon=True, alpha=.6)\n",
    "cbar = mp.colorbar(cs,location='bottom',pad=\"5%\")\n",
    "\n",
    "_ = plt.title('%s at %s' % (DATA_TYPE, DATE_SEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = make_map(gfs[DATA_TYPE].bounding_box)\n",
    "mp.shadedrelief()\n",
    "\n",
    "values = calc_mean(DATA_TYPE)\n",
    "lats, lons = gfs[DATA_TYPE].bounding_box.make_grid()\n",
    "\n",
    "cs = mp.contourf(lons, lats , values, latlon=True, alpha=.6)\n",
    "cbar = mp.colorbar(cs,location='bottom',pad=\"5%\")\n",
    "#cbar.set_label('Kelvin (degrees)')\n",
    "\n",
    "_ = plt.title('Mean %s' % DATA_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation for left neighbor\n",
    "cor = calc_cor((0, 0, 0), (1, 0, -1), DATA_TYPE)\n",
    "\n",
    "mp = make_map(gfs[DATA_TYPE].bounding_box)\n",
    "mp.shadedrelief()\n",
    "\n",
    "values = cor\n",
    "lats, lons = gfs[DATA_TYPE].bounding_box.make_grid()\n",
    "\n",
    "cs = mp.contourf(lons[:,1:], lats[:,1:] ,values[:,1:], latlon=True, alpha=.6)\n",
    "cbar = mp.colorbar(cs,location='bottom',pad=\"5%\")\n",
    "cbar.set_label('Correlation (0 to 1)')\n",
    "\n",
    "_ = plt.title('Left Correlation of %s' % DATA_TYPE)\n",
    "\n",
    "print 'Min=%f, Max=%f, Mean=%f' % (np.min(values), np.max(values), np.mean(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation for top neighbor\n",
    "cor = calc_cor((1, 0, -1), (0, 0, 0), DATA_TYPE)\n",
    "\n",
    "mp = make_map(gfs[DATA_TYPE].bounding_box)\n",
    "mp.shadedrelief()\n",
    "\n",
    "values = cor\n",
    "lats, lons = gfs[DATA_TYPE].bounding_box.make_grid()\n",
    "\n",
    "cs = mp.contourf(lons[1:, :], lats[1:, :] ,values[1:, :], latlon=True, alpha=.6)\n",
    "cbar = mp.colorbar(cs,location='bottom',pad=\"5%\")\n",
    "cbar.set_label('Correlation (0 to 1)')\n",
    "\n",
    "_ = plt.title('Top Correlation of %s' % DATA_TYPE)\n",
    "\n",
    "print 'Min=%f, Max=%f, Mean=%f' % (np.min(values), np.max(values), np.mean(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation for bottom neighbor\n",
    "cor = calc_cor((0, -1, 1), (0, 0, 0), DATA_TYPE)\n",
    "\n",
    "mp = make_map(gfs[DATA_TYPE].bounding_box)\n",
    "mp.shadedrelief()\n",
    "\n",
    "values = cor\n",
    "lats, lons = gfs[DATA_TYPE].bounding_box.make_grid()\n",
    "\n",
    "cs = mp.contourf(lons[:-1, :], lats[:-1, :] ,values[:-1, :], latlon=True, alpha=.6)\n",
    "cbar = mp.colorbar(cs,location='bottom',pad=\"5%\")\n",
    "cbar.set_label('Correlation (0 to 1)')\n",
    "\n",
    "_ = plt.title('Bottom Correlation of %s' % DATA_TYPE)\n",
    "\n",
    "print 'Min=%f, Max=%f, Mean=%f' % (np.min(values), np.max(values), np.mean(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
