{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GFS Open Question Exploration (v2.0)\n",
    "Casey A Graff\n",
    "\n",
    "August 11th, 2017\n",
    "\n",
    "**Now using re-fetched gfs data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REP_DIR = \"/home/cagraff/Documents/dev/fire_prediction/\"\n",
    "SRC_DIR = REP_DIR + 'src/'\n",
    "DATA_DIR = REP_DIR + 'data/'\n",
    "\n",
    "# Load system-wide packages\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import datetime as dt\n",
    "import pytz\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load project packages\n",
    "os.chdir(SRC_DIR)\n",
    "from features.loaders import load_gfs_weather\n",
    "from helper import date_util as du\n",
    "from visualization.mapping import make_map\n",
    "from visualization.stats import calc_mean, calc_cor\n",
    "from helper.geometry import latlonrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "sys.path.append(SRC_DIR+'helper')\n",
    "gfs = load_gfs_weather(os.path.join(DATA_DIR, 'interim/gfs/weather/weather_gfs_alaska_2007-2016.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print gfs.cubes.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing files\n",
    "How many files are missing and is there a temporal pattern to the missing days?\n",
    "\n",
    "### Number of missings files per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2007, 2017)\n",
    "\n",
    "missing = []\n",
    "for year in years:\n",
    "    sel = gfs['temperature'].filter_dates(du.DatetimeMeasurement(dt.datetime(year,1,1, tzinfo=pytz.UTC)), du.DatetimeMeasurement(dt.datetime(year, 12, 31, tzinfo=pytz.UTC)))\n",
    "    \n",
    "    files_missing = len([v for v in sel.values[0,0,:] if np.isnan(v)])\n",
    "    files_present = sel.shape[2] - files_missing\n",
    "        \n",
    "    missing.append((year, files_present, files_missing))\n",
    "\n",
    "# Missing total\n",
    "missing.append(('Total', sum([x[1] for x in missing]), sum([x[2] for x in missing])))\n",
    "print tabulate(missing, headers=['Year', 'Present', 'Missing'])\n",
    "\n",
    "print '\\nPercentage missing is {}%'.format(missing[-1][2]/(.01*missing[-1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of missings files per year (within fire season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2007, 2017)\n",
    "season = ((5,14), (8,31))\n",
    "\n",
    "print 'Fire Season:', season[0], 'to', season[1], '\\n'\n",
    "\n",
    "# Missing per year\n",
    "missing = []\n",
    "\n",
    "for year in years:\n",
    "    sel = gfs['temperature'].filter_dates(du.DatetimeMeasurement(dt.datetime(year, season[0][0], season[0][1], tzinfo=pytz.UTC)), du.DatetimeMeasurement(dt.datetime(year, season[1][0], season[1][1], tzinfo=pytz.UTC)))\n",
    "    \n",
    "    files_missing = len([v for v in sel.values[0,0,:] if np.isnan(v)])\n",
    "    files_present = sel.shape[2] - files_missing\n",
    "        \n",
    "    missing.append((year, files_present, files_missing))\n",
    "\n",
    "# Missing total\n",
    "missing.append(('Total', sum([x[1] for x in missing]), sum([x[2] for x in missing])))\n",
    "print tabulate(missing, headers=['Year', 'Present', 'Missing'])\n",
    "\n",
    "print '\\nPercentage missing is {}%'.format(missing[-1][2]/(.01*missing[-1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of missing days per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2007, 2017)\n",
    "months = range(1, 13)\n",
    "\n",
    "MONTH_IND = 0\n",
    "PRESENT_IND = 1\n",
    "MISSING_IND = 2\n",
    "\n",
    "missing = np.zeros((12,3), dtype=np.int32)\n",
    "missing[:, MONTH_IND] = months\n",
    "# TODO: Missing a few days        \n",
    "for year in years:\n",
    "    for month in months:\n",
    "        month_num_days = du.days_per_month(month, du.is_leap_year(year))\n",
    "        sel = gfs['temperature'].filter_dates(du.DatetimeMeasurement(dt.datetime(year, month, 1, tzinfo=pytz.UTC)), du.DatetimeMeasurement(dt.datetime(year, month, month_num_days, tzinfo=pytz.UTC)))\n",
    "        total += sel.shape[2]\n",
    "        files_missing = len([v for v in sel.values[0,0,:] if np.isnan(v)])\n",
    "        files_present = sel.shape[2] - files_missing\n",
    "        \n",
    "        missing[month-1,PRESENT_IND] += files_present\n",
    "        missing[month-1,MISSING_IND] += files_missing\n",
    "\n",
    "\n",
    "# Missing total\n",
    "present_total = np.sum(missing[:, PRESENT_IND])\n",
    "missing_total = np.sum(missing[:, MISSING_IND])\n",
    "missing = list(missing)\n",
    "missing.append(['Total', present_total, missing_total])\n",
    "\n",
    "print tabulate(missing, headers=['Month', 'Present', 'Missing'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Missing Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2011, 2011)\n",
    "grib_file_fmt = \"gfsanl_4_%s%.2d%.2d_%.2d%.2d_%.3d.grb2\"\n",
    "# TODO: Update\n",
    "missing_files = []\n",
    "for year in years:\n",
    "    all_dates = [d for d in du.daterange(dt.datetime(year,1,1, tzinfo=pytz.UTC), dt.datetime(year+1, 1, 1, tzinfo=pytz.UTC), increment=dt.timedelta(hours=6))]\n",
    "\n",
    "    for date in all_dates:\n",
    "        offsets_found = gfs['temperature'].get_attribute_for_date('offsets', date)\n",
    "        \n",
    "        gribs_found = [grib_file_fmt % (year, date.month, date.day, date.hour, date.minute, offset.seconds/3600) for offset in offsets_found]\n",
    "        gribs_expected = [grib_file_fmt % (year, date.month, date.day, date.hour, date.minute, offset) for offset in (0, 3, 6)]\n",
    "\n",
    "        missing_files += list(set(gribs_expected).difference(set(gribs_found)))\n",
    "\n",
    "print len(missing_files), missing_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Correlation of Measurements\n",
    "\n",
    "Do adjacent pixel have a high correlation between measurements? If there is sufficient variability it may be useful to perform linear interpolation between neighboring cells when calculating the weather variables for a fire event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = ['total_precipitation','u_wind_component', 'v_wind_component', 'temperature', 'humidity']\n",
    "DATA_TYPE = data_types[3]\n",
    "DATE_SEL = dt.datetime(2009, 3, 5, 18, tzinfo=pytz.UTC)\n",
    "OFFSET_SEL = 2\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10,15]\n",
    "\n",
    "mp = make_map(gfs[DATA_TYPE].bounding_box)\n",
    "mp.shadedrelief()\n",
    "\n",
    "latlon = [ll for ll in latlonrange(gfs[DATA_TYPE].bounding_box, .5, .5)]\n",
    "lats,lons = zip(*latlon)\n",
    "\n",
    "_ = mp.scatter(lons, lats ,30, latlon=True, marker='o', color='b')\n",
    "_ = plt.title('GFS Meaurement Points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "    \n",
    "mp = make_map(gfs[DATA_TYPE].bounding_box)\n",
    "mp.shadedrelief()\n",
    "\n",
    "values = gfs[DATA_TYPE].get_values_for_date(DATE_SEL)[:,:,OFFSET_SEL]\n",
    "lats, lons = gfs[DATA_TYPE].bounding_box.make_grid()\n",
    "\n",
    "cs = mp.contourf(lons, lats , values, latlon=True, alpha=.6)\n",
    "cbar = mp.colorbar(cs,location='bottom',pad=\"5%\")\n",
    "\n",
    "_ = plt.title('%s at %s' % (DATA_TYPE, DATE_SEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = make_map(gfs[DATA_TYPE].bounding_box)\n",
    "mp.shadedrelief()\n",
    "\n",
    "values = calc_mean(gfs[DATA_TYPE].values, gfs[DATA_TYPE].values.shape)\n",
    "lats, lons = gfs[DATA_TYPE].bounding_box.make_grid()\n",
    "\n",
    "cs = mp.contourf(lons, lats , values, latlon=True, alpha=.6)\n",
    "cbar = mp.colorbar(cs,location='bottom',pad=\"5%\")\n",
    "#cbar.set_label('Kelvin (degrees)')\n",
    "\n",
    "_ = plt.title('Mean %s' % DATA_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation for left neighbor\n",
    "cor = calc_cor(gfs[DATA_TYPE].values, gfs[DATA_TYPE].values.shape, (0, 0, 0), (1, 0, -1))\n",
    "\n",
    "mp = make_map(gfs[DATA_TYPE].bounding_box)\n",
    "mp.shadedrelief()\n",
    "\n",
    "values = cor\n",
    "lats, lons = gfs[DATA_TYPE].bounding_box.make_grid()\n",
    "\n",
    "cs = mp.contourf(lons[:,1:], lats[:,1:] ,values[:,1:], latlon=True, alpha=.6)\n",
    "cbar = mp.colorbar(cs,location='bottom',pad=\"5%\")\n",
    "cbar.set_label('Correlation (0 to 1)')\n",
    "\n",
    "_ = plt.title('Left Correlation of %s' % DATA_TYPE)\n",
    "\n",
    "print 'Min=%f, Max=%f, Mean=%f' % (np.min(values), np.max(values), np.mean(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation for top neighbor\n",
    "cor = calc_cor(gfs[DATA_TYPE].values, gfs[DATA_TYPE].values.shape, (1, 0, -1), (0, 0, 0))\n",
    "\n",
    "mp = make_map(gfs[DATA_TYPE].bounding_box)\n",
    "mp.shadedrelief()\n",
    "\n",
    "values = cor\n",
    "lats, lons = gfs[DATA_TYPE].bounding_box.make_grid()\n",
    "\n",
    "cs = mp.contourf(lons[1:, :], lats[1:, :] ,values[1:, :], latlon=True, alpha=.6)\n",
    "cbar = mp.colorbar(cs,location='bottom',pad=\"5%\")\n",
    "cbar.set_label('Correlation (0 to 1)')\n",
    "\n",
    "_ = plt.title('Top Correlation of %s' % DATA_TYPE)\n",
    "\n",
    "print 'Min=%f, Max=%f, Mean=%f' % (np.min(values), np.max(values), np.mean(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation for bottom neighbor\n",
    "cor = calc_cor(gfs[DATA_TYPE].values, gfs[DATA_TYPE].values.shape, (0, -1, 1), (0, 0, 0))\n",
    "\n",
    "mp = make_map(gfs[DATA_TYPE].bounding_box)\n",
    "mp.shadedrelief()\n",
    "\n",
    "values = cor\n",
    "lats, lons = gfs[DATA_TYPE].bounding_box.make_grid()\n",
    "\n",
    "cs = mp.contourf(lons[:-1, :], lats[:-1, :] ,values[:-1, :], latlon=True, alpha=.6)\n",
    "cbar = mp.colorbar(cs,location='bottom',pad=\"5%\")\n",
    "cbar.set_label('Correlation (0 to 1)')\n",
    "\n",
    "_ = plt.title('Bottom Correlation of %s' % DATA_TYPE)\n",
    "\n",
    "print 'Min=%f, Max=%f, Mean=%f' % (np.min(values), np.max(values), np.mean(values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring +0, +3, +6 Offset Files\n",
    "\n",
    "### Are the +3 and +6 offsets for instantaneous variables different than +0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TYPE = 'humidity'\n",
    "lat = 0\n",
    "lon = 0\n",
    "all_equals = []\n",
    "for i in range(0, gfs.shape[2]-2, 3):\n",
    "    equals_3_offset = gfs[DATA_TYPE].values[lat,lon,i] == gfs[DATA_TYPE].values[lat,lon,i+1]\n",
    "    equals_6_offset = gfs[DATA_TYPE].values[lat,lon,i] == gfs[DATA_TYPE].values[lat,lon,i+2]\n",
    "    equals_both = equals_3_offset and equals_6_offset\n",
    "    one_is_nan = np.isnan(gfs[DATA_TYPE].values[lat,lon,i]) or np.isnan(gfs[DATA_TYPE].values[lat,lon,i+1]) or np.isnan(gfs[DATA_TYPE].values[lat,lon,i+2])\n",
    "    if not one_is_nan:\n",
    "        all_equals.append(equals_both)\n",
    "\n",
    "print DATA_TYPE, all(all_equals), np.mean(all_equals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantaneous variables at +0, +3 and +6 are not always equal.**\n",
    "\n",
    "### Is the +6 and +0 from the next time stamp equal for instantaneous variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TYPE = 'humidity'\n",
    "lat = 0\n",
    "lon = 0\n",
    "all_equals = []\n",
    "for i in range(3, gfs.shape[2], 3):\n",
    "    is_equal = gfs[DATA_TYPE].values[lat,lon,i] == gfs[DATA_TYPE].values[lat,lon,i-1]\n",
    "    one_is_nan = np.isnan(gfs[DATA_TYPE].values[lat,lon,i]) or np.isnan(gfs[DATA_TYPE].values[lat,lon,i-1])\n",
    "    if not one_is_nan:\n",
    "        all_equals.append(is_equal)\n",
    "        \n",
    "    #print gfs.dates[i].get().hour, gfs.dates[i].get_offset(), gfs[DATA_TYPE].values[lat,lon,i], gfs.dates[i-1].get().hour,gfs.dates[i-1].get_offset(), gfs[DATA_TYPE].values[lat,lon,i-1]\n",
    "\n",
    "print DATA_TYPE, all(all_equals), np.mean(all_equals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No, +6 from current time and +0 from next six hour interval are usually not equal.**\n",
    "\n",
    "### Is +3 precipitation always less than or equal to +6?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TYPE = 'total_precipitation'\n",
    "lat = 0\n",
    "lon = 0\n",
    "all_equals = []\n",
    "for i in range(1, gfs.shape[2]-1, 3):\n",
    "    is_lt_or_equal = gfs[DATA_TYPE].values[lat,lon,i] <= gfs[DATA_TYPE].values[lat,lon,i+1]\n",
    "    one_is_nan = np.isnan(gfs[DATA_TYPE].values[lat,lon,i]) or np.isnan(gfs[DATA_TYPE].values[lat,lon,i+1])\n",
    "    if not one_is_nan:\n",
    "        all_equals.append(is_lt_or_equal)\n",
    "        #if not is_lt_or_equal: print gfs.dates[i].get(), gfs.dates[i].get_offset(), gfs[DATA_TYPE].values[lat,lon,i], gfs.dates[i+1].get(),gfs.dates[i+1].get_offset(), gfs[DATA_TYPE].values[lat,lon,i+1]\n",
    "\n",
    "print DATA_TYPE, all(all_equals), np.mean(all_equals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the vast majority of cases +3 rain is less than or equal to +6, but not always.**\n",
    "\n",
    "Perhaps the outliers are caused by measurement or model error.\n",
    "\n",
    "## Daily Temperature Timings\n",
    "Exploring the change in temperature within a day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfs_proc = load_gfs_weather(os.path.join(DATA_DIR, 'interim/gfs/weather_proc/weather_proc_gfs_alaska_2007-2016.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = dt.datetime(2010,7,22,0,0, tzinfo=pytz.UTC) # 2010, 7, 15; 2010 7 22\n",
    "lat, lon = 61.2, -149.9 # anchorage\n",
    "from features import fire_weather_integration as fwi\n",
    "\n",
    "lat_ind, lon_ind = fwi.FireWeatherIntegration(None).get_latlon_index(gfs_proc, lat, lon)\n",
    "day = fwi.FireWeatherIntegration(None).get_date_index(gfs_proc, date)\n",
    "\n",
    "val1 = np.mean(gfs_proc['temperature'].values[lat_ind,lon_ind,day+0])\n",
    "val2 = np.mean(gfs_proc['temperature'].values[lat_ind,lon_ind,day+1])\n",
    "val3 = np.mean(gfs_proc['temperature'].values[lat_ind,lon_ind,day+2])\n",
    "val4 = np.mean(gfs_proc['temperature'].values[lat_ind,lon_ind,day+3])\n",
    "date1 = gfs_proc['temperature'].dates[day+0].astimezone(du.TrulyLocalTzInfo(lon, du.round_to_nearest_quarter_hour))\n",
    "date2 = gfs_proc['temperature'].dates[day+1].astimezone(du.TrulyLocalTzInfo(lon, du.round_to_nearest_quarter_hour))\n",
    "date3 = gfs_proc['temperature'].dates[day+2].astimezone(du.TrulyLocalTzInfo(lon, du.round_to_nearest_quarter_hour))\n",
    "date4 = gfs_proc['temperature'].dates[day+3].astimezone(du.TrulyLocalTzInfo(lon, du.round_to_nearest_quarter_hour))\n",
    "\n",
    "import pytz\n",
    "print date1, val1\n",
    "print date2, val2\n",
    "print date3, val3\n",
    "print date4, val4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
