{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Model Testing (v1.0)\n",
    "\n",
    "Casey A Graff\n",
    "\n",
    "November 3rd, 2017\n",
    "\n",
    "Now using updated $X_{cluster}$ format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REP_DIR = \"/home/cagraff/Documents/dev/fire_prediction/\"\n",
    "SRC_DIR = REP_DIR + 'src/'\n",
    "DATA_DIR = REP_DIR + 'data/'\n",
    "\n",
    "# Load system-wide packages\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import datetime as dt\n",
    "import bisect\n",
    "plt.rcParams['figure.figsize'] = [15,7]\n",
    "%matplotlib inline\n",
    "\n",
    "# Load project packages\n",
    "os.chdir(SRC_DIR)\n",
    "from features.loaders import load_integrated_df\n",
    "from features.loaders import load_fire_cube\n",
    "import models.poisson_regression as pr\n",
    "import models.linear_regression as lr\n",
    "import models.regional_summation_regression as rsr\n",
    "import models.evaluation as ev\n",
    "from models import metrics\n",
    "from helper import date_util as du\n",
    "from helper import df_util as dfu\n",
    "from helper import preprocessing as pp\n",
    "from features import generate_grid as gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(SRC_DIR+'features')\n",
    "int_5km_10days_14_1k_df = load_integrated_df(os.path.join(DATA_DIR, 'interim/integrated/fire_weather/fire_weather_integrated_gfs_4_modis_5km_10days_1400_1k_alaska_2007-2016.pkl'))\n",
    "int_5km_10days_14_2k_df = load_integrated_df(os.path.join(DATA_DIR, 'interim/integrated/fire_weather/fire_weather_integrated_gfs_4_modis_5km_10days_1400_2k_alaska_2007-2016.pkl'))\n",
    "int_5km_10days_14_3k_df = load_integrated_df(os.path.join(DATA_DIR, 'interim/integrated/fire_weather/fire_weather_integrated_gfs_4_modis_5km_10days_1400_3k_alaska_2007-2016.pkl'))\n",
    "#int_5km_10days_14_4k_df = load_integrated_df(os.path.join(DATA_DIR, 'interim/integrated/fire_weather/fire_weather_integrated_gfs_4_modis_5km_10days_1400_4k_alaska_2007-2016.pkl'))\n",
    "#int_5km_10days_14_5k_df = load_integrated_df(os.path.join(DATA_DIR, 'interim/integrated/fire_weather/fire_weather_integrated_gfs_4_modis_5km_10days_1400_5k_alaska_2007-2016.pkl'))\n",
    "\n",
    "fire_cube, fire_cube_dates = load_fire_cube(os.path.join(DATA_DIR, 'interim/modis/fire_cube/fire_cube_modis_alaska_2007_2016.pkl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_5km_10days_14_1k_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(ev)\n",
    "reload(gg)\n",
    "reload(pp)\n",
    "def prep(X, t_k, years=None):\n",
    "    X = pp.standardize_covariates(X, ['temperature', 'humidity', 'wind', 'rain'])   \n",
    "    X = X.assign(year=map(lambda x: x.year, X.date_local))\n",
    "    if years: X = X[X.year.isin(years)]\n",
    "    X_t = pp.add_autoregressive_col(X, t_k)\n",
    "    \n",
    "    return X_t\n",
    "\n",
    "def train_poisson(X_t, t_k, covariates):  \n",
    "    prm = pr.PoissonRegressionModel(t_k=t_k, covariates=covariates)\n",
    "    results, years = ev.cross_validation_years(prm, X_t)\n",
    "    \n",
    "    preds = [x[0] for x in results]\n",
    "    info  = [x[1] for x in results]\n",
    "    \n",
    "    return preds, info, years\n",
    "\n",
    "def cluster_targets(res_info, X):\n",
    "    targets = []\n",
    "    for y_info in res_info:\n",
    "        y_test = np.zeros(len(y_info))\n",
    "        for i,inf in enumerate(y_info):\n",
    "            y_test[i] = X[(X.date_local==inf[0]) & (X.cluster_id==inf[3])].iloc[0].num_det_target\n",
    "        targets.append(y_test)\n",
    "        \n",
    "    return targets\n",
    "\n",
    "def pred_to_grid(preds, info):\n",
    "    grids = []\n",
    "    dates_all = []\n",
    "    for pred,inf in zip(preds,info):\n",
    "        grid,dates = gg.gen_grid_predictions(pred, inf)\n",
    "        grids.append(grid)\n",
    "        dates_all.append(dates)\n",
    "        \n",
    "    return grids, dates_all\n",
    "\n",
    "def grid_targets(dates_all, fire_cube, fire_cube_dates, t_k):\n",
    "    # TODO: Only need the dates from predictions\n",
    "    targets = []\n",
    "    for dates in dates_all:\n",
    "        start_ind, end_ind = bisect.bisect_left(fire_cube_dates, dates[0]), bisect.bisect_left(fire_cube_dates, dates[-1])\n",
    "        target_grid = fire_cube[:,:,start_ind:end_ind+1]\n",
    "        # Append t_k days of all zeros, then \"shift\" values forward by t_k days (dropping first t_k days)\n",
    "        shape = np.shape(target_grid)[:2]+(t_k,)\n",
    "        target_grid = np.concatenate((target_grid, np.zeros(shape)), axis=2)\n",
    "        target_grid = target_grid[:,:,t_k:]\n",
    "        \n",
    "        targets.append(target_grid)\n",
    "        \n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Move this to fire_weather_integration\n",
    "X_pp = []\n",
    "X_pp.append(prep(int_5km_10days_14_1k_df.copy(), 1))\n",
    "X_pp.append(prep(int_5km_10days_14_2k_df.copy(), 2))\n",
    "X_pp.append(prep(int_5km_10days_14_3k_df.copy(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(results, t_k_arr, metric):\n",
    "    #plt.plot(t_k_arr, map(lambda x: metric(*x), results['baseline_mean']), \"yv--\", label=\"Baseline (Mean)\", linewidth=2)\n",
    "    #plt.plot(t_k_arr, map(lambda x: metric(*x), results['baseline_median']), \"cv--\", label=\"Baseline (Median)\", linewidth=2)\n",
    "    #plt.plot(t_k_arr, map(lambda x: metric(*x), results['baseline_prev']), \"kv--\", label=\"Baseline (Previous)\", linewidth=2)\n",
    "    plt.plot(t_k_arr, map(lambda x: metric(*x), results['auto']), \"gs--\", label=\"Autoregression\", linewidth=2)\n",
    "    plt.plot(t_k_arr, map(lambda x: metric(*x), results['temp_humid']), \"r^--\", label=\"Temp/hum\", linewidth=2)\n",
    "    plt.plot(t_k_arr, map(lambda x: metric(*x), results['all']), \"bo--\", label=\"All weather\", linewidth=2)\n",
    "    \n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    lgd = plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.xlabel(\"Day of forecast (k)\")\n",
    "    plt.xticks(t_k_arr+1)\n",
    "    plt.ylabel(metric.__name__)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_k_arr = np.arange(1, 4)\n",
    "\n",
    "results = defaultdict(list)\n",
    "\n",
    "reload(ev)\n",
    "reload(rsr)\n",
    "\n",
    "for t_k in t_k_arr:\n",
    "    print 'Starting %d' % t_k    \n",
    "    results['auto'].append(train_poisson(X_pp[t_k-1], t_k, []))\n",
    "    \n",
    "    results['temp_humid'].append(train_poisson(X_pp[t_k-1], t_k, ['temperature', 'humidity']))\n",
    "\n",
    "    results['all'].append(train_poisson(X_pp[t_k-1], t_k, ['temperature', 'humidity', 'wind', 'rain']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cluster(results, X_pp, t_k_arr):\n",
    "    results_new = defaultdict(list)\n",
    "    for t_k in t_k_arr:\n",
    "        info = results['auto'][t_k-1][1]\n",
    "        targets_c = cluster_targets(info, X_pp[t_k-1])\n",
    "    \n",
    "        for k in results:            \n",
    "            results_new[k].append(np.concatenate(zip(targets_c, results[k][t_k-1][0]), axis=1))\n",
    "    \n",
    "    return results_new\n",
    "\n",
    "def evaluate_grid(results, fire_cube, fire_cube_dates, t_k_arr):\n",
    "    results_new = defaultdict(list)\n",
    "    _, dates_all = pred_to_grid(results['auto'][0][0], results['auto'][0][1])\n",
    "    for t_k in t_k_arr:\n",
    "        targets_g = grid_targets(dates_all, fire_cube, fire_cube_dates, t_k)\n",
    "    \n",
    "        for k in results:\n",
    "            predict_g, dates_all = pred_to_grid(results[k][t_k-1][0], results[k][t_k-1][1])\n",
    "            results_new[k].append(map(lambda x: x.flatten(), np.concatenate(zip(targets_g, predict_g), axis=3)))\n",
    "    \n",
    "    return results_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print np.shape(results['auto'][t_k-1][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_new = evaluate_cluster(results, X_pp, t_k_arr)\n",
    "plot(results_new, t_k_arr, metrics.mean_absolute_error)\n",
    "plt.show()\n",
    "plot(results_new, t_k_arr, metrics.root_mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_new = evaluate_grid(results, fire_cube, fire_cube_dates, t_k_arr)\n",
    "plot(results_new, t_k_arr, metrics.mean_absolute_error)\n",
    "plt.show()\n",
    "plot(results_new, t_k_arr, metrics.root_mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_k_arr = np.arange(0, 3)\n",
    "\n",
    "results = defaultdict(list)\n",
    "\n",
    "reload(ev)\n",
    "reload(rsr)\n",
    "\n",
    "for t_k in t_k_arr:\n",
    "    print 'Starting %d' % t_k\n",
    "    X_ = pp2(int_5km_10days_14_1k_df, t_k)\n",
    "    X_region = rsr.RegionalSummationModel(t_k+1, None, None, None).build_regional_data(X_)\n",
    "    print 'Mean Daily Det %f' % np.mean(X_region.num_det)\n",
    "    \n",
    "    results['baseline_mean'].append((X_region.num_det_target, np.mean(X_region.num_det)))\n",
    "    results['baseline_median'].append((X_region.num_det_target, np.median(X_region.num_det)))\n",
    "    results['baseline_prev'].append((X_region.num_det_target, X_region.num_det))\n",
    "\n",
    "    results['auto'].append(train_regional(X_p[t_k], t_k, [], X_))\n",
    "    \n",
    "    results['temp_humid'].append(train_regional(X_p[t_k], t_k, ['temperature', 'humidity'], X_))\n",
    "\n",
    "    results['all'].append(train_regional(X_p[t_k], t_k, ['temperature', 'humidity', 'wind', 'rain'], X_))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
